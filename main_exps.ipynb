{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6dXqkFpp2pS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression, load_diabetes, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "print(\"ğŸš€ Linear Regression Optimizer Comparison - FIXED\")\n",
        "\n",
        "# CHOOSE DATASET HERE (change this option)\n",
        "DATASET_CHOICE = 0  # 0=Synthetic, 1=Diabetes, 2=California Housing, 3=make_regression\n",
        "\n",
        "if DATASET_CHOICE == 0:\n",
        "    # Synthetic data (2D for visualization)\n",
        "    n_samples, n_features = 200, 2\n",
        "    noise_level = 0.1\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(n_samples, n_features)\n",
        "    true_coef = np.array([2.0, -1.5])\n",
        "    y = X @ true_coef + noise_level * np.random.randn(n_samples)\n",
        "    print(\"ğŸ“Š Synthetic  200 samples, 2 features\")\n",
        "\n",
        "elif DATASET_CHOICE == 1:\n",
        "    # Diabetes dataset (real medical data)\n",
        "    data = load_diabetes()\n",
        "    X, y = data.data, data.target\n",
        "    print(f\"ğŸ“Š Diabetes dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "\n",
        "elif DATASET_CHOICE == 2:\n",
        "    # California Housing (modern replacement for Boston)\n",
        "    data = fetch_california_housing()\n",
        "    X, y = data.data, data.target\n",
        "    print(f\"ğŸ“Š California Housing: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "\n",
        "elif DATASET_CHOICE == 3:\n",
        "    # Synthetic regression (scikit-learn generator)s\n",
        "    X, y = make_regression(n_samples=500, n_features=10, noise=0.1, random_state=42)\n",
        "    print(\"ğŸ“Š Synthetic regression: 500 samples, 10 features\")\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "print(f\"âœ… Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "print(f\"ğŸ“ˆ Data ready for optimization!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFaGU1d3p90r"
      },
      "outputs": [],
      "source": [
        "def optimize_all(X_train, y_train,\n",
        "                 MAX_ITER_ADAGRAD=10000, LR_ADAGRAD=0.03, EARLY_STOP_ADAGRAD=True,  # MAX 10K!\n",
        "                 MAX_ITER_NEWTON=20, EARLY_STOP_NEWTON=True):\n",
        "    n, p = X_train.shape\n",
        "    Xb = np.c_[np.ones((n, 1)), X_train]\n",
        "\n",
        "    # 1. NORMAL EQUATION\n",
        "    theta_normal = np.linalg.pinv(Xb.T @ Xb) @ Xb.T @ y_train\n",
        "    normal_loss = 0.5 * np.mean((Xb @ theta_normal - y_train)**2)\n",
        "\n",
        "    # 2. ADAGRAD (MAX 10K!)\n",
        "    print(f\"ğŸš€ AdaGrad: MAX={MAX_ITER_ADAGRAD} iter, LR={LR_ADAGRAD}\")\n",
        "    theta_adagrad = np.zeros(Xb.shape[1])\n",
        "    g2_acc = np.zeros_like(theta_adagrad) * 1e-6\n",
        "    adagrad_losses = [float('inf')]\n",
        "    adagrad_traj = [theta_adagrad.copy()]\n",
        "    adagrad_iters_used = 0\n",
        "\n",
        "    for i in range(MAX_ITER_ADAGRAD):  # Up to 10K!\n",
        "        grad = Xb.T @ (Xb @ theta_adagrad - y_train) / n\n",
        "        g2_acc += grad**2\n",
        "        theta_adagrad -= LR_ADAGRAD * grad / (np.sqrt(g2_acc) + 1e-8)\n",
        "        loss = 0.5 * np.mean((Xb @ theta_adagrad - y_train)**2)\n",
        "        adagrad_losses.append(loss)\n",
        "        adagrad_traj.append(theta_adagrad.copy())\n",
        "        adagrad_iters_used = i + 1\n",
        "\n",
        "        if EARLY_STOP_ADAGRAD and i > 100 and abs(adagrad_losses[-1] - adagrad_losses[-2]) < 1e-7:\n",
        "            print(f\"   â„¹ï¸ AdaGrad STOPPED at {i+1}/{MAX_ITER_ADAGRAD}\")\n",
        "            break\n",
        "\n",
        "    adagrad_traj = np.array(adagrad_traj)\n",
        "    adagrad_losses = np.array(adagrad_losses)\n",
        "\n",
        "    # 3. NEWTON\n",
        "    print(f\"ğŸš€ Newton: MAX={MAX_ITER_NEWTON} iter\")\n",
        "    theta_newton = np.zeros(Xb.shape[1])\n",
        "    newton_losses = [float('inf')]\n",
        "    newton_traj = [theta_newton.copy()]\n",
        "    newton_iters_used = 0\n",
        "\n",
        "    for i in range(MAX_ITER_NEWTON):\n",
        "        residuals = Xb @ theta_newton - y_train\n",
        "        grad = Xb.T @ residuals / n\n",
        "        hessian = Xb.T @ Xb / n\n",
        "        delta = np.linalg.pinv(hessian) @ grad\n",
        "        theta_newton -= delta\n",
        "        loss = 0.5 * np.mean(residuals**2)\n",
        "        newton_losses.append(loss)\n",
        "        newton_traj.append(theta_newton.copy())\n",
        "        newton_iters_used = i + 1\n",
        "\n",
        "        if EARLY_STOP_NEWTON and loss < 1e-8:\n",
        "            print(f\"   â„¹ï¸ Newton STOPPED at {i+1}/{MAX_ITER_NEWTON}\")\n",
        "            break\n",
        "\n",
        "    newton_traj = np.array(newton_traj)\n",
        "    newton_losses = np.array(newton_losses)\n",
        "\n",
        "    return {\n",
        "        'normal': theta_normal, 'normal_loss': normal_loss, 'normal_iters': 1,\n",
        "        'adagrad': theta_adagrad, 'adagrad_losses': adagrad_losses,\n",
        "        'adagrad_traj': adagrad_traj, 'adagrad_iters': adagrad_iters_used,\n",
        "        'newton': theta_newton, 'newton_losses': newton_losses,\n",
        "        'newton_traj': newton_traj, 'newton_iters': newton_iters_used\n",
        "    }\n",
        "\n",
        "# ğŸ›ï¸ MAX 10K CONTROL PANEL\n",
        "ADAGRAD_CONFIG = {\n",
        "    'max_iter': 10000,    # MAX 10K! (change: 100, 500, 1000, 2500, 10000)\n",
        "    'lr': 0.03,\n",
        "    'early_stop': True\n",
        "}\n",
        "\n",
        "NEWTON_CONFIG = {\n",
        "    'max_iter': 20,       # (change: 3, 5, 10, 20)\n",
        "    'early_stop': True\n",
        "}\n",
        "\n",
        "# RUN\n",
        "results = optimize_all(X_train, y_train,\n",
        "                      MAX_ITER_ADAGRAD=ADAGRAD_CONFIG['max_iter'],\n",
        "                      LR_ADAGRAD=ADAGRAD_CONFIG['lr'],\n",
        "                      EARLY_STOP_ADAGRAD=ADAGRAD_CONFIG['early_stop'],\n",
        "                      MAX_ITER_NEWTON=NEWTON_CONFIG['max_iter'],\n",
        "                      EARLY_STOP_NEWTON=NEWTON_CONFIG['early_stop'])\n",
        "\n",
        "print(f\"\\nâœ… MAX 10K VERIFIED:\")\n",
        "print(f\"   AdaGrad: {results['adagrad_iters']}/{ADAGRAD_CONFIG['max_iter']} iterations\")\n",
        "print(f\"   Newton:  {results['newton_iters']}/{NEWTON_CONFIG['max_iter']} iterations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Bh4T3yqBMB"
      },
      "outputs": [],
      "source": [
        "# Test MSE\n",
        "Xb_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
        "mse_normal = mean_squared_error(y_test, Xb_test @ results['normal'])\n",
        "mse_adagrad = mean_squared_error(y_test, Xb_test @ results['adagrad'])\n",
        "mse_newton = mean_squared_error(y_test, Xb_test @ results['newton'])\n",
        "\n",
        "# FULL COMPARISON TABLE (Iterations REQUIRED, not max!)\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Method': ['Normal Equation', 'AdaGrad', 'Newton Method'],\n",
        "    'Final Train Loss': [results['normal_loss'],\n",
        "                        results['adagrad_losses'][-1],\n",
        "                        results['newton_losses'][-1]],\n",
        "    'Test MSE': [mse_normal, mse_adagrad, mse_newton],\n",
        "    'Iterations Required': [results['normal_iters'],\n",
        "                          results['adagrad_iters'],\n",
        "                          results['newton_iters']],  # â† ACTUAL iterations used!\n",
        "})\n",
        "\n",
        "print(\"\\nğŸ“Š COMPLETE COMPARISON TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.round(6).to_string(index=False))\n",
        "print(f\"\\nğŸ† BEST METHOD: {comparison_df.loc[comparison_df['Test MSE'].idxmin(), 'Method']}\")\n",
        "print(f\"ğŸ“ˆ Best Test MSE: {comparison_df['Test MSE'].min():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2KNUgbWPI7z"
      },
      "outputs": [],
      "source": [
        "# 1. MSE BAR CHART\n",
        "plt.figure(figsize=(10, 6))\n",
        "methods = ['Normal Eq.', 'AdaGrad', 'Newton']\n",
        "mses = [mse_normal, mse_adagrad, mse_newton]\n",
        "colors = ['#1f77b4', '#2ca02c', '#ff7f0e']\n",
        "bars = plt.bar(methods, mses, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "plt.ylabel('Test MSE', fontsize=14, fontweight='bold')\n",
        "plt.title('Test Performance Comparison', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.ylim(0, max(mses)*1.1)\n",
        "\n",
        "for bar, mse in zip(bars, mses):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height*1.02, f'{mse:.4f}',\n",
        "             ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"ğŸ“Š 1/2: Test MSE Comparison âœ“\")\n",
        "\n",
        "# 2. CONVERGENCE CURVES (log scale)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(results['adagrad_losses'][1:], 'g-', lw=4, marker='o', label='AdaGrad', markersize=6)\n",
        "plt.semilogy(results['newton_losses'][1:], 'orange', lw=4, marker='s', label='Newton', markersize=6)\n",
        "plt.axhline(results['normal_loss'], color='blue', ls='--', lw=3,\n",
        "           label=f'Normal Eq. (loss={results[\"normal_loss\"]:.2e})')\n",
        "plt.xlabel('Iteration', fontweight='bold')\n",
        "plt.ylabel('Loss (log scale)', fontweight='bold')\n",
        "plt.title('Convergence Speed Comparison', fontweight='bold', pad=20)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"ğŸ“ˆ 2/2: Convergence Curves âœ“\")\n",
        "\n",
        "\n",
        "print(\"ğŸŒŠ 3/3: Loss Contours âœ“\")\n",
        "print(\"âœ¨ All visualizations complete!!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gqE2eahcYYRZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib\n",
        "\n",
        "# ğŸ”§ FIX SIZE LIMIT\n",
        "matplotlib.rcParams['animation.embed_limit'] = 128  # 128MB\n",
        "\n",
        "print(\"ğŸ¬ Cell 5: 3D + 2D Animations (200 FRAMES - 10K Progress!)\")\n",
        "\n",
        "if results['adagrad_traj'].shape[1] < 3:\n",
        "    print(\"âš ï¸ Run Cell 1 with DATASET_CHOICE=0!\")\n",
        "else:\n",
        "    # Loss surface\n",
        "    w0_range = np.linspace(-3, 3, 50)\n",
        "    w1_range = np.linspace(-3, 3, 50)\n",
        "    T0, T1 = np.meshgrid(w0_range, w1_range)\n",
        "    L_surf = 0.5 * (T0**2 + 4*T1**2)\n",
        "    true_theta = np.array([0.0, 0.0])\n",
        "\n",
        "    colors = {\n",
        "        'AdaGrad': '#2ca02c', 'Newton': '#ff7f0e', 'Normal': '#1f77b4'\n",
        "    }\n",
        "\n",
        "    def animate_convergence(view='3D'):\n",
        "        fig = plt.figure(figsize=(12, 9))\n",
        "\n",
        "        if view == '3D':\n",
        "            ax = fig.add_subplot(111, projection='3d')\n",
        "            surf = ax.plot_surface(T0, T1, L_surf, cmap='viridis', alpha=0.7)\n",
        "            ax.set_zlabel('Loss', fontweight='bold')\n",
        "            ax.scatter(true_theta[0], true_theta[1], 0, color='black', s=200, marker='*', label='True Min')\n",
        "        else:\n",
        "            ax = fig.add_subplot(111)\n",
        "            cs = ax.contourf(T0, T1, L_surf, levels=30, cmap='viridis', alpha=0.8)\n",
        "            ax.contour(T0, T1, L_surf, levels=10, colors='white', alpha=0.7, linewidths=0.8)\n",
        "            ax.scatter(true_theta[0], true_theta[1], color='black', s=200, marker='*', label='True Min')\n",
        "            plt.colorbar(cs, ax=ax, label='Loss', shrink=0.8)\n",
        "\n",
        "        ax.set_xlabel('Î¸â‚€ (bias)', fontweight='bold')\n",
        "        ax.set_ylabel('Î¸â‚ (w1)', fontweight='bold')\n",
        "        ax.set_title(f'Optimizer Paths - {len(results[\"adagrad_traj\"])-1} Total Iterations ({view})',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "\n",
        "        # Text: 3D vs 2D\n",
        "        if view == '3D':\n",
        "            iter_text = ax.text2D(0.02, 0.98, \"\", transform=ax.transAxes,\n",
        "                                 fontsize=14, color='darkred', fontweight='bold',\n",
        "                                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.9))\n",
        "        else:\n",
        "            iter_text = ax.text(0.02, 0.98, \"\", transform=ax.transAxes,\n",
        "                               fontsize=14, color='darkred', fontweight='bold',\n",
        "                               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.9))\n",
        "\n",
        "        # Lines + points\n",
        "        lines, points = {}, {}\n",
        "        for name, color in colors.items():\n",
        "            if view == '3D':\n",
        "                l, = ax.plot([], [], [], color=color, lw=3, label=name)\n",
        "                p, = ax.plot([], [], [], color=color, marker='o', ms=8)\n",
        "            else:\n",
        "                l, = ax.plot([], [], color=color, lw=3, label=name)\n",
        "                p, = ax.plot([], [], color=color, marker='o', ms=8)\n",
        "            lines[name], points[name] = l, p\n",
        "\n",
        "        ax.legend(loc='upper right', frameon=True, fancybox=True, shadow=True, fontsize=11)\n",
        "\n",
        "        def init():\n",
        "            for l, p in zip(lines.values(), points.values()):\n",
        "                if view == '3D':\n",
        "                    l.set_data([], []); l.set_3d_properties([])\n",
        "                    p.set_data([], []); p.set_3d_properties([])\n",
        "                else:\n",
        "                    l.set_data([], []); p.set_data([], [])\n",
        "            iter_text.set_text(\"\")\n",
        "            return list(lines.values()) + list(points.values()) + [iter_text]\n",
        "\n",
        "        def animate(frame):\n",
        "            total_iter = len(results['adagrad_traj']) - 1\n",
        "\n",
        "            # ğŸ”¥ SMART SAMPLING: Map 200 frames to full 10K trajectory\n",
        "            real_iter = int((frame / 199) * total_iter)  # Scale to full trajectory\n",
        "\n",
        "            for name in colors:\n",
        "                if name == 'Normal':\n",
        "                    traj = np.array([[results['normal'][1], results['normal'][2]]])\n",
        "                else:\n",
        "                    traj_len = min(real_iter + 1, len(results[f'{name.lower()}_traj']))\n",
        "                    traj = results[f'{name.lower()}_traj'][:traj_len, 1:3]\n",
        "\n",
        "                if view == '3D':\n",
        "                    z_vals = 0.5 * (traj[:,0]**2 + 4*traj[:,1]**2)\n",
        "                    lines[name].set_data(traj[:, 0], traj[:, 1])\n",
        "                    lines[name].set_3d_properties(z_vals)\n",
        "                    points[name].set_data([traj[-1, 0]], [traj[-1, 1]])\n",
        "                    points[name].set_3d_properties([z_vals[-1]])\n",
        "                else:\n",
        "                    lines[name].set_data(traj[:, 0], traj[:, 1])\n",
        "                    points[name].set_data([traj[-1, 0]], [traj[-1, 1]])\n",
        "\n",
        "            iter_text.set_text(f\"Iter: {real_iter+1}/{total_iter}\")\n",
        "            return list(lines.values()) + list(points.values()) + [iter_text]\n",
        "\n",
        "        # âœ… 200 FRAMES = Fast + Shows full 10K progress!\n",
        "        max_frames = 200\n",
        "        ani = animation.FuncAnimation(fig, animate, frames=max_frames,\n",
        "                                     init_func=init, interval=100, blit=False, repeat=True)\n",
        "        plt.close(fig)\n",
        "        return ani\n",
        "\n",
        "    # RUN\n",
        "    total_iter = len(results['adagrad_traj']) - 1\n",
        "    print(f\"\\nğŸŒŸ 3D ANIMATION (200 frames showing {total_iter} iterations):\")\n",
        "    ani_3d = animate_convergence('3D')\n",
        "    display(HTML(ani_3d.to_jshtml()))\n",
        "\n",
        "    print(f\"\\nğŸŒŸ 2D ANIMATION (200 frames showing {total_iter} iterations):\")\n",
        "    ani_2d = animate_convergence('2D')\n",
        "    display(HTML(ani_2d.to_jshtml()))\n",
        "\n",
        "    print(f\"\\nâœ… PERFECT! 200 smooth frames map to your full {total_iter} iterations!\")\n",
        "    print(\"ğŸŸ¢ Green path shows COMPLETE 10K convergence!\")\n",
        "\n",
        "\n",
        "    # === UPLOAD TO GITHUB FROM COLAB ===\n",
        "!git config --global user.email \"1032251810@tcetmumbai.in\"\n",
        "!git config --global user.name \"Aayush Kasurde\"\n",
        "\n",
        "# Clone your new repo\n",
        "!git clone https://github.com/Aayush9305/LR-Optimization-Comparison-\n",
        "%cd LR-Optimization-Comparison\n",
        "\n",
        "# Create structure\n",
        "!mkdir src notebooks data outputs\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
