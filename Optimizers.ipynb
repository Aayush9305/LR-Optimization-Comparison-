def optimize_all(X_train, y_train,
                 MAX_ITER_ADAGRAD=10000, LR_ADAGRAD=0.03, EARLY_STOP_ADAGRAD=True,  # MAX 10K!
                 MAX_ITER_NEWTON=20, EARLY_STOP_NEWTON=True):
    n, p = X_train.shape
    Xb = np.c_[np.ones((n, 1)), X_train]

    # 1. NORMAL EQUATION
    theta_normal = np.linalg.pinv(Xb.T @ Xb) @ Xb.T @ y_train
    normal_loss = 0.5 * np.mean((Xb @ theta_normal - y_train)**2)

    # 2. ADAGRAD (MAX 10K!)
    print(f"ğŸš€ AdaGrad: MAX={MAX_ITER_ADAGRAD} iter, LR={LR_ADAGRAD}")
    theta_adagrad = np.zeros(Xb.shape[1])
    g2_acc = np.zeros_like(theta_adagrad) * 1e-6
    adagrad_losses = [float('inf')]
    adagrad_traj = [theta_adagrad.copy()]
    adagrad_iters_used = 0

    for i in range(MAX_ITER_ADAGRAD):  # Up to 10K!
        grad = Xb.T @ (Xb @ theta_adagrad - y_train) / n
        g2_acc += grad**2
        theta_adagrad -= LR_ADAGRAD * grad / (np.sqrt(g2_acc) + 1e-8)
        loss = 0.5 * np.mean((Xb @ theta_adagrad - y_train)**2)
        adagrad_losses.append(loss)
        adagrad_traj.append(theta_adagrad.copy())
        adagrad_iters_used = i + 1

        if EARLY_STOP_ADAGRAD and i > 100 and abs(adagrad_losses[-1] - adagrad_losses[-2]) < 1e-7:
            print(f"   â„¹ï¸ AdaGrad STOPPED at {i+1}/{MAX_ITER_ADAGRAD}")
            break

    adagrad_traj = np.array(adagrad_traj)
    adagrad_losses = np.array(adagrad_losses)

    # 3. NEWTON
    print(f"ğŸš€ Newton: MAX={MAX_ITER_NEWTON} iter")
    theta_newton = np.zeros(Xb.shape[1])
    newton_losses = [float('inf')]
    newton_traj = [theta_newton.copy()]
    newton_iters_used = 0

    for i in range(MAX_ITER_NEWTON):
        residuals = Xb @ theta_newton - y_train
        grad = Xb.T @ residuals / n
        hessian = Xb.T @ Xb / n
        delta = np.linalg.pinv(hessian) @ grad
        theta_newton -= delta
        loss = 0.5 * np.mean(residuals**2)
        newton_losses.append(loss)
        newton_traj.append(theta_newton.copy())
        newton_iters_used = i + 1

        if EARLY_STOP_NEWTON and loss < 1e-8:
            print(f"   â„¹ï¸ Newton STOPPED at {i+1}/{MAX_ITER_NEWTON}")
            break

    newton_traj = np.array(newton_traj)
    newton_losses = np.array(newton_losses)

    return {
        'normal': theta_normal, 'normal_loss': normal_loss, 'normal_iters': 1,
        'adagrad': theta_adagrad, 'adagrad_losses': adagrad_losses,
        'adagrad_traj': adagrad_traj, 'adagrad_iters': adagrad_iters_used,
        'newton': theta_newton, 'newton_losses': newton_losses,
        'newton_traj': newton_traj, 'newton_iters': newton_iters_used
    }

# ğŸ›ï¸ MAX 10K CONTROL PANEL
ADAGRAD_CONFIG = {
    'max_iter': 10000,    # MAX 10K! (change: 100, 500, 1000, 2500, 10000)
    'lr': 0.03,
    'early_stop': True
}

NEWTON_CONFIG = {
    'max_iter': 20,       # (change: 3, 5, 10, 20)
    'early_stop': True
}

# RUN
results = optimize_all(X_train, y_train,
                      MAX_ITER_ADAGRAD=ADAGRAD_CONFIG['max_iter'],
                      LR_ADAGRAD=ADAGRAD_CONFIG['lr'],
                      EARLY_STOP_ADAGRAD=ADAGRAD_CONFIG['early_stop'],
                      MAX_ITER_NEWTON=NEWTON_CONFIG['max_iter'],
                      EARLY_STOP_NEWTON=NEWTON_CONFIG['early_stop'])

print(f"\nâœ… MAX 10K VERIFIED:")
print(f"   AdaGrad: {results['adagrad_iters']}/{ADAGRAD_CONFIG['max_iter']} iterations")
print(f"   Newton:  {results['newton_iters']}/{NEWTON_CONFIG['max_iter']} iterations")
